---
title: "Draftsim analysis"
output: html_notebook
---

Some R code to analyze Draftsim drafting results.

```{r}
require(dplyr)
require(ggplot2)
require(tidyr)
load("C:/Users/Sysadmin/Documents/draftsim/pairs variable M19 part 1 v3.RData")
```

Let me start with describing the data; the exact structure of this data is not that important for the analysis, below, but I will be referring to it, so we can as well describe it. The data used for this workbook comes from the analysis of 48137 drafts of M19 dataset that users ran on draftsim web-site (http://draftsim.com) in August 2018. I then loaded this data in R, and wrote a simple code that went through final draft piles one by one, and calculate the total number of cases that every two cards from the set (i and j) were drafted together:

* pairs - a matrix of all card co-occurrences in different drafts

To make calculations simpler, I also counted the total number of times each card was drafted at all:

* freq - the total number of times each card was drafted

For each card, I also calculated the average point at which it was picked. I called the variable "rank", but actually it's not ranked, and also it's reversed: the higher this value, the later in a draft this card was picked, on average. For technical reasons, for this value I only used the first booster out of 3 (at the moment of this writing, we had some issues with storing the exact sequence of card picks for later boosters), but it OK. Also, for this "rank" value I didn't correct for synergies; that is, I did not try to adjust the value in any way when the same red card was picked late in a blue draft, but early on in a red draft. I assume here that with 50 000 total drafts, these effects will largely average out, although of course there are better ways to look at this problem.

* rank - the order of card picks

Finally, to visualize the cards nicely I also loaded a database that for each card stores some useful information about it:

* db - info about every card from this particular set

```{r}
names(db)
```

The fields that are important here are the id, the name, and the casting cost. All names I turned lowercase, replaced spaces with underscores, and purged commas, to make analysis easier. And the "cost" field I split into separate costs of each mana (WUBRG) using this command below. This was important, to classify cards into colors and guilds:

```{r}
db <- mutate(db,w = grepl("W",cost)*1,
                u = grepl("U",cost)*1,
                b = grepl("B",cost)*1,
                r = grepl("R",cost)*1,
                g = grepl("G",cost)*1) # Number of symbols for each mana in cost
```

All other fields may be ignored.

So, let's look at the "pairs" matrix. Again, it contains the data about every co-occurrence of every two cards in thefinal draft piles. That is, if out of entire 48137 drafts, cards i and j were drafted together only once, the ij element of this matrix will be equal to 1. If these two cards were co-drafted 100 times, the value will be equal to 100. Here's how it looks like right after it was built by the loop running through all draft results:

```{r}
image(pairs,axes=F,col = grey(seq(0, 1, length = 256)))
```
Here's the distribution of the upper part of "pairs" values:

(Tehnically this formula is a bit wrong, as it double-counts cards on the diagonal p_ii, for cards that were drafted in 2 copies, but it's close enough to the truth for a quick and dirty histogram)

```{r}
ggplot() + geom_histogram(aes(x=c(pairs+t(pairs))),bins=200) + theme_bw()
```
Some cards were drafted together thousands of times, but most were only drafted together a few hundred times or so. Here's a zoom-in on the leftmost part of the histogram.

```{r}
temp <- c(pairs+t(pairs))
ggplot() + geom_histogram(aes(x=temp[temp<1000]),bins=200) + theme_bw()
```
Out of curiosity, what's the rarest pair of cards drafted?

```{r}
min(temp)
```
```{r}
ij <- which(pairs == min(temp), arr.ind = TRUE)
ij
```
```{r}
db[ij[1],1]
```
```{r}
db[ij[2],1]
```
A 2-color mythic and a rare of a non-matching color. No wonder.



Well, let's go further. The next  thing we need to do now is to make this matrix symmetrical, and then normalize it. Let's make a copy of "pairs" variable, and move from relative numbers to probabilities:

p_ij = n_ij/nCards

Finally, let's move from mutual probabilities to relative probabilities. That is, let's assume that card i can be found in drafted hands with probability p_i = n_i/nCards , and similar probability for card j is denoted as p_j. Then, if the cards are drafted independently from each other (that is, if the player doesn't care about the presence of card i in their hand, while drafting card j, and vice versa), then we'll have p_ij = p_i * p_j

In practice some cards are of course drafted together more often, and so p_ij > p_i * p_j, while some cards are drafted together more rarely. We can quantify this preference (or anti-prefereence) as a coefficient a, with  the following formula:

a_ij = p_ij/(p_i * p_j)

In practice I tracked the number (not probability) that each pair of cards ended up in the same pile n_ij, as well as the number of times card i was present in the final pile n_i , and the same for card j: n_j. If a card was present in the final pile twice, it was accounted twice, both in the n_ij and in n_i. For these counts, in case of independence, we can expect

n_ij/nDrafts = n_i/nDrafts * n_j/Drafts * pileSize*(pileSize-1), 

where pileSize = 14*3 and so

k_ij = n_ij/(n_i * n_j) * nDrafts * 14*3


```{r}
nCards <- nrow(db)                 # Repeat, in case the data was reloaded
p <- pairs                         # Make a copy
for(jCard in 1:nCards) {
    for(iCard in 1:jCard) {
      p[iCard,jCard] <- p[iCard,jCard]/(freq[iCard]*freq[jCard])*nDrafts*14*3
      p[jCard,iCard] <- p[iCard,jCard]
    }
}

```

Now if we look at this matrix, it will look different:

```{r}
image(p,axes=F,col = grey(seq(0, 1, length = 256)))
```

What is the highest synergy, if calculated that way? 

```{r}
max(p)
```

And which pair of cards turned to be the most "synergistic"?

```{r}
ij <- which(p == max(p), arr.ind = TRUE)
db[ij[1],1]
```
```{r}
db[ij[2],1]
```

Curiously, it's a synergy of Elvish Clancaller with itself. The card is not surprizing, it's an elf lord after all, but it is curious that with this calculation, rares clearly have an edge over commons and non-commons, as while nobody would ever pass over two cool synergistic rares, drafted in two consecutive boosters, people would sometimes pass over a synergistic common or uncommon.

In case you were wondering, the second most synergistic pair in this set is a combo of "blanchwood_armor" with "druid_of_horns" (a cool enchantment and a cool creature that benefits from enchantments) that have a relative preference of 71.33 .

Now, time to go from these relative mutual probabilities to distances, to make a nice plot. Mutual probabilities are high if two cards are often drafted together, but we want a distance-like value that would be low if two cards are often drafted together. There are many formulas that one can use here, but we'll go with a simplest one:

```{r}
dist <- (1-0.99*p/max(p))
```

I use 0.99 to keep the closest cards still separated a bit, just in case.

Now, there is a whole set of methods that are willing to take a matrix of pairwise distances and try to find an arrangement of points in a space of some dimension that would approximate these pairwise distances as well as possible. The idea behind the methods is simple: throw some points out there, and move them around, until some measure of alingment is maximised. Say, one can try to maxime Pearson correlation between pairwise distances in this new low-dimensional space, and "input pairwise distances".

The simplest approach from this family is called MDS, or multidimensional scaling. Let's apply it to our data:

```{r}
ndim <- 2 
fit <- cmdscale(dist,eig=TRUE, k=ndim)
scale <- data.frame(x=fit$points[,1],y=fit$points[,2])
```

Now we have a dataset with x and y points for each card, and the distribution of this points approximates target pairwise distances as well as possible (in some sense, at least). The points are nice, but how to interpret them?

```{r}
ggplot(scale,aes(x,y)) + theme_bw() + geom_point() + coord_fixed()
```
We'll have to draw from card database "db", and combine this dataset with newly calculated x and y points. If you remember, in my "db" set I calculated mana costs for each mana color, and stored them in fields names WUBRG respectively. I'll now need to somehow transform these WUBRG fields into one field with "color" values. My solution (below) is a bit weird, but hey, it works.

```{r}
scale <- mutate(scale,id=1:nrow(scale))       # Add a column of ids (as the sequence of cards matched ids from db)
scale <- inner_join(scale,db,by="id")         # Bring all fields from db to scale, useful and useless alike
scale <- mutate(scale,color_n=0)              # What is the color of this card, if coded as a number?
scale <- mutate(scale,color_n=w*1+u*2+b*3+r*4+g*5) 
scale <- mutate(scale,color_n=color_n*((w+u+b+r+g)==1)+6*((w+u+b+r+g)>1))

# Now we need to translate these numbers into factors. I'll use an SQL-style approach; maybe there's a simpler way
colorList <- data.frame(color_n=c(0,1,2,3,4,5,6),
                        color=c("0","W","U","B","R","G","M"))
colorList$color = factor(colorList$color,levels=c("0","W","U","B","R","G","M"),ordered=T) # Don't order alphabetically!
scale <- inner_join(scale,colorList,by="color_n") # Done
```

Time to plot same picture as above, but with proper colors!

```{r}
myColors <- c("gray","tan2","blue","black","red","green","purple")
ggplot(scale,aes(x,y,color=color)) + theme_bw() + geom_point() + coord_fixed() +
  scale_color_manual(values=myColors) + xlab('') + ylab('')
```

That's a nice and interesting plot! Please read our [Post at Draftsim blog](https://draftsim.com/blog/draft-data-analysis/) for a full detailed description of how to read it, and how to interpret it.

To make a plot with labels, I used a package ggrepel. This plot takes a while to generate, and it looks horrible in a workbook, but IRL one can save it at 2000x2000 px, as I did for the blog post, which made it quite readable.

```{r}
require(ggrepel)
ggplot(scale) + theme_bw() + geom_point(aes(x,y,color=color)) +
  scale_color_manual(values=c("gray","tan2","blue","black","red","green","purple")) +
  geom_text_repel(aes(x,y,label=name),size=2,box.padding = 0.01, point.padding = 0.01) +
  xlab('') + ylab('') 
```

Now, let's look at the pick order. We'll need to attach the "rank" variable to our data frame, and then we can visualize it:

```{r}
scale <- mutate(scale,rank=rank)
ggplot(scale,aes(color,rank(rank),color=color)) + theme_bw() + geom_point() +
  geom_text(aes(label=name),color='black',size=2,hjust="left",position = position_nudge(x = 0.05)) +
  scale_color_manual(values=c("gray","tan2","blue","black","red","green","purple"))
```
Again, this plot doesn't necessarily look good at this resolution, but it would look better if scaled properly.

Finally, what about 3d plots for the paper? To get a 3d plot, I would reset ndim to 3:
```{r}
ndim <- 3
```

then rerun the mds analysis (but now in 3D rather than 2D), and visualize it with RGL package:

```{r}
require(rgl)
require(car)
require(magick)

plot3d(scale$x,scale$y,scale$z,surface=F,col=myColors[scale$color],
       xlab="", ylab="", zlab="", size=4, alpha=0.8)
movie3d(spin3d(axis = c(0,0,1), rpm = 2), duration=10, type="gif", dir=myFolder)
```

I won't be running this code here, but you can see a beautiful rotating 3D plot [in the original article](https://draftsim.com/blog/draft-data-analysis/) 